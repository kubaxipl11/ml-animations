//! Demo: XOR Problem
//!
//! The XOR (exclusive or) problem is a classic test for neural networks.
//! It's non-linearly separable, so a single-layer perceptron cannot solve it.
//! A network with at least one hidden layer is required.

use mini_nn::{Network, Activation, Loss, Optimizer, Trainer, TrainingConfig};
use mini_nn::data::generate_xor;

fn main() {
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!("            Mini-NN: XOR Problem Demo");
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
    println!();
    
    // Generate XOR dataset
    println!("ðŸ“Š Generating XOR dataset...");
    let dataset = generate_xor(1000);
    println!("   Samples: {}", dataset.n_samples);
    println!("   Features: {}", dataset.x.shape().1);
    println!();
    
    // Show the XOR truth table
    println!("ðŸ“‹ XOR Truth Table:");
    println!("   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
    println!("   â”‚  A  â”‚  B  â”‚ A XOR B â”‚");
    println!("   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
    println!("   â”‚  0  â”‚  0  â”‚    0    â”‚");
    println!("   â”‚  0  â”‚  1  â”‚    1    â”‚");
    println!("   â”‚  1  â”‚  0  â”‚    1    â”‚");
    println!("   â”‚  1  â”‚  1  â”‚    0    â”‚");
    println!("   â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
    println!();
    
    // Create the network
    // Architecture: 2 â†’ 8 â†’ 8 â†’ 1
    println!("ðŸ§  Creating network...");
    let mut network = Network::new()
        .add_dense(2, 8)
        .add_activation(Activation::ReLU)
        .add_dense(8, 8)
        .add_activation(Activation::ReLU)
        .add_dense(8, 1)
        .add_activation(Activation::Sigmoid);
    
    network.summary();
    println!();
    
    // Training configuration
    let config = TrainingConfig {
        epochs: 100,
        batch_size: 32,
        validation_split: 0.2,
        shuffle: true,
        early_stopping_patience: 20,
        verbose: true,
    };
    
    // Train the network
    println!("ðŸŽ¯ Training network...");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    
    let trainer = Trainer::new(config);
    let history = trainer.fit(
        &mut network,
        &dataset.x,
        &dataset.y,
        Loss::BinaryCrossEntropy,
        Optimizer::adam(0.01),
    );
    
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    println!();
    
    // Results
    println!("ðŸ“ˆ Training Results:");
    println!("   Final train loss: {:.4}", history.train_loss.last().unwrap_or(&0.0));
    println!("   Final train acc:  {:.1}%", history.train_accuracy.last().unwrap_or(&0.0) * 100.0);
    println!("   Best val loss:    {:.4}", history.best_val_loss().unwrap_or(0.0));
    println!("   Best val acc:     {:.1}%", history.best_val_accuracy().unwrap_or(0.0) * 100.0);
    println!();
    
    // Test on the 4 XOR cases
    println!("ðŸ” Testing on XOR truth table:");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    
    use ndarray::arr2;
    use mini_nn::Tensor;
    
    let test_cases: [([f64; 2], f64); 4] = [
        ([0.0, 0.0], 0.0),
        ([0.0, 1.0], 1.0),
        ([1.0, 0.0], 1.0),
        ([1.0, 1.0], 0.0),
    ];
    
    let mut correct = 0;
    for (inputs, expected) in &test_cases {
        let x = Tensor::new(arr2(&[[inputs[0], inputs[1]]]));
        let pred = network.predict(&x);
        let pred_val = pred.data[[0, 0]];
        let pred_class: f64 = if pred_val >= 0.5 { 1.0 } else { 0.0 };
        let is_correct = (pred_class - expected).abs() < 0.5;
        
        println!(
            "   Input: [{:.0}, {:.0}] â†’ Pred: {:.4} ({:.0}) | Expected: {:.0} | {}",
            inputs[0], inputs[1],
            pred_val, pred_class,
            expected,
            if is_correct { "âœ“" } else { "âœ—" }
        );
        
        if is_correct {
            correct += 1;
        }
    }
    
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    println!("   Accuracy: {}/4 ({:.0}%)", correct, correct as f64 * 25.0);
    println!();
    
    // Explanation
    println!("ðŸ’¡ Why XOR Matters:");
    println!("   XOR is the simplest problem that requires a hidden layer.");
    println!("   A single perceptron can only create linear decision boundaries,");
    println!("   but XOR needs a non-linear boundary. Our network with ReLU");
    println!("   activations learns this non-linear mapping successfully!");
    println!();
    println!("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");
}
